{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import augmentation\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "from scipy.stats.mstats import linregress\n",
    "from clint.textui import progress\n",
    "from random import uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Dense, Flatten, MaxPooling2D, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import normalize\n",
    "\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(CSV):\n",
    "    df = pd.read_csv(CSV, delim_whitespace=False, header=0, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['test', 'validate', 'train']\n",
    "root_dir = 'Images'\n",
    "source_dir = 'images'\n",
    "\n",
    "for c in class_names:\n",
    "    os.mkdir(os.path.join(root_dir, c))\n",
    "    \n",
    "for c in class_names:\n",
    "    images = [x for x in os.listdir(source_dir)]\n",
    "    if c == 'train':\n",
    "        selected_images = random.sample(images, len(images))\n",
    "    else: \n",
    "        selected_images = random.sample(images, 630)\n",
    "    for image in selected_images:\n",
    "        source_path = os.path.join(source_dir, image)\n",
    "        target_path = os.path.join(root_dir, c, image)\n",
    "        shutil.move(source_path, target_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_into_memory(DIR, ANNO, ATTRIBUTE, normalize=True, rollaxis=True):\n",
    "    if DIR[:-1] != '/': DIR += '/'\n",
    "    df = parse_csv(ANNO)\n",
    "    files = filter(lambda x: x in df.index.values, os.listdir(DIR))\n",
    "    X, y = [], []\n",
    "    for image_path in files:\n",
    "        img = imageio.imread(DIR + image_path)\n",
    "        if normalize: img = img.astype('float32') / 255.\n",
    "        if rollaxis: img.shape = (1,150,130)\n",
    "        else: img.shape = (150,130,1)\n",
    "        X.append(img)\n",
    "        mu = df[ATTRIBUTE][image_path]\n",
    "        y.append(mu)\n",
    "    y = np.array(y)\n",
    "    y = y - min(y)\n",
    "    y = np.float32(y / max(y))\n",
    "\n",
    "    x, y = np.array(X), np.array(y)\n",
    "    print(\"Loaded {} images into memory\", len(y))\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(x, y, batch_size, space, sampling_factor=3, sampling_intercept=2, weighted_sampling=False, augment=False):\n",
    "    if weighted_sampling:\n",
    "\n",
    "        def get_bin_index(bin_edges, value):\n",
    "            for index in range(len(bin_edges)):\n",
    "                if value <= bin_edges[index + 1]:\n",
    "                    return index\n",
    "            return index\n",
    "\n",
    "        hist, bin_edges = np.histogram(y, bins=200)\n",
    "        most = max(hist)\n",
    "        hist_norm = hist / float(most)\n",
    "        hist_norm_inv = (1. - hist_norm)\n",
    "        hist_norm_inv = hist_norm_inv ** sampling_factor + 10 ** -sampling_intercept\n",
    "        probs = []\n",
    "        for y_ in y:\n",
    "            index = get_bin_index(bin_edges, y_)\n",
    "            probs.append(hist_norm_inv[index])\n",
    "\n",
    "        should_sample = lambda pctprob: uniform(0,1) <= pctprob\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        Xbatch, ybatch, in_batch = [], [], 0\n",
    "        while in_batch < batch_size:\n",
    "            if weighted_sampling:\n",
    "                while not should_sample(probs[i]):\n",
    "                    i = i + 1 if i + 1 < len(y) else 0\n",
    "            if augment: x_ = augmentation.applyRandomAugmentation(x[i], space)\n",
    "            else: x_ = x[i]\n",
    "            x_ = x_.astype('float32') / 255.\n",
    "            x_.shape = (1, 150, 130)\n",
    "            Xbatch.append(x_)\n",
    "            ybatch.append(y[i])\n",
    "            in_batch += 1\n",
    "            i = i + 1 if i + 1 < len(y) else 0\n",
    "\n",
    "        yield np.array(Xbatch), np.array(ybatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_variant(space):\n",
    "    model = Sequential()\n",
    "\n",
    "    for outputs in space['conv0filters']:\n",
    "        model.add(Convolution2D(outputs, (3, 3), padding='same', input_shape=(1, 150, 130), kernel_initializer='glorot_uniform',\n",
    "                                use_bias=True, activation='relu'))\n",
    "        model.add(Convolution2D(outputs, (3, 3), padding='same', use_bias=True, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    for outputs in space['conv1filters']:\n",
    "        model.add(Convolution2D(outputs, 3, 3, padding='same', kernel_initializer='glorot_uniform', use_bias=True, activation='relu'))\n",
    "        model.add(Convolution2D(outputs, 3, 3, padding='same', kernel_initializer='glorot_uniform', use_bias=True, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    for outputs in space['conv2filters']:\n",
    "        model.add(Convolution2D(outputs, 3, 3, padding='same', kernel_initializer='glorot_uniform', use_bias=True, activation='relu'))\n",
    "        model.add(Convolution2D(outputs, 3, 3, padding='same', kernel_initializer='glorot_uniform', use_bias=True, activation='relu'))\n",
    "        model.add(Convolution2D(outputs, 3, 3, padding='same', kernel_initializer='glorot_uniform', use_bias=True, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for _ in range(int(space['num_fc'])):\n",
    "        model.add(Dense(int(space['fcoutput']), kernel_initializer='glorot_uniform', use_bias=True, activation='relu'))\n",
    "        model.add(Dropout(space['dropout']))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='glorot_uniform', use_bias=True))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Rsquared(y, predicted):\n",
    "    m, b, r, p, e = linregress(y=y, x=predicted)\n",
    "    r2 = r**2\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, x, y):\n",
    "    predicted = model.predict(x)\n",
    "    r2 = get_Rsquared(y, predicted)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Xtrain, ytrain, Xtrain_norm, ytrain_norm, Xvalidate, yvalidate, space):\n",
    "    import sys\n",
    "    from keras.optimizers import RMSprop\n",
    "    from keras.callbacks import Callback\n",
    "\n",
    "    class CorrelationEarlyStopping(Callback):\n",
    "        def __init__(self, monitor='validate', patience=0, delta=.001):\n",
    "            \"\"\"\n",
    "            :param monitor: 'validate' or 'train'\n",
    "            :param patience: how many epochs to wait\n",
    "            :param delta: by how much the monitored value has to be greater than the last maximum\n",
    "            \"\"\"\n",
    "            self.rvalues = {'train': [], 'validate': []}\n",
    "            self.monitor = monitor  # validate, train\n",
    "            self.patience = patience\n",
    "            self.delta = delta\n",
    "            self.wait = 0\n",
    "            self.best = 0\n",
    "            self.num_epochs = 0\n",
    "            self.best_model = None\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            r2 = get_metrics(self.model, x=Xtrain_norm, y=ytrain_norm)\n",
    "            self.rvalues['train'].append(r2)\n",
    "            r2 = get_metrics(self.model, x=Xvalidate, y=yvalidate)\n",
    "            self.rvalues['validate'].append(r2)\n",
    "            print ('\\n\\tTrain r2: {}\\n\\tValidate r2: {}\\n'.format(self.rvalues['train'][-1], self.rvalues['validate'][-1]))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if self.rvalues[self.monitor][-1] - self.delta >= self.best:\n",
    "                self.best = self.rvalues[self.monitor][-1]\n",
    "                self.wait = 0\n",
    "                self.num_epochs = epoch\n",
    "                self.best_model = self.model\n",
    "            else:\n",
    "                if self.wait >= self.patience:\n",
    "                    self.num_epochs = epoch - self.patience\n",
    "                    self.model.stop_training = True\n",
    "                else:\n",
    "                    self.num_epochs = epoch\n",
    "                    self.wait += 1\n",
    "\n",
    "    model = vgg_variant(space)\n",
    "    lr = 10**(-space['learning_rate'])\n",
    "    rmsprop = RMSprop(lr=lr, rho=0.9, epsilon=1e-08)\n",
    "    model.compile(loss='mean_squared_error', optimizer=rmsprop)\n",
    "    monitor = CorrelationEarlyStopping(monitor='validate', patience=6, delta=0.01)\n",
    "    gen = data_generator(Xtrain, ytrain, batch_size=space['batch_size'], space=space,\n",
    "                         weighted_sampling=space['weighted_sampling'], augment=space['augment'],\n",
    "                         sampling_factor=space['sampling_factor'], sampling_intercept=space['sampling_intercept'])\n",
    "    model_history = model.fit_generator(gen, space['samples_per_epoch'], 50, 1, [monitor], (Xvalidate, yvalidate))\n",
    "    return monitor.best_model, monitor.rvalues, model_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Data\n",
      "Loaded {} images into memory 5041\n",
      "Loading Train Data Again\n",
      "Loaded {} images into memory 5041\n",
      "Loading Validation Data\n",
      "Loaded {} images into memory 630\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 1, 150, 64)        74944     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 150, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 1, 75, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 75, 64)         36928     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 75, 64)         36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 38, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 13, 128)        73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 1, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 5,423,233\n",
      "Trainable params: 5,423,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-8-05b67cae84c6>:51: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0352\n",
      "\tTrain r2: 0.21903291514561907\n",
      "\tValidate r2: 0.18129242663697423\n",
      "\n",
      "4000/4000 [==============================] - 929s 232ms/step - loss: 0.0352 - val_loss: 0.0304\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0236\n",
      "\tTrain r2: 0.34874804046309066\n",
      "\tValidate r2: 0.2414735074224592\n",
      "\n",
      "4000/4000 [==============================] - 886s 222ms/step - loss: 0.0236 - val_loss: 0.0277\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0179\n",
      "\tTrain r2: 0.3893228742934502\n",
      "\tValidate r2: 0.19215258222818016\n",
      "\n",
      "4000/4000 [==============================] - 752s 188ms/step - loss: 0.0179 - val_loss: 0.0323\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0133\n",
      "\tTrain r2: 0.45574405210722235\n",
      "\tValidate r2: 0.18862705638785382\n",
      "\n",
      "4000/4000 [==============================] - 634s 158ms/step - loss: 0.0133 - val_loss: 0.0344\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0097\n",
      "\tTrain r2: 0.4869192156704256\n",
      "\tValidate r2: 0.19202741323672257\n",
      "\n",
      "4000/4000 [==============================] - 634s 158ms/step - loss: 0.0097 - val_loss: 0.0291\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0074\n",
      "\tTrain r2: 0.5405002954043436\n",
      "\tValidate r2: 0.14541178684470824\n",
      "\n",
      "4000/4000 [==============================] - 633s 158ms/step - loss: 0.0074 - val_loss: 0.0309\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0058\n",
      "\tTrain r2: 0.5643290159460463\n",
      "\tValidate r2: 0.1912644249623793\n",
      "\n",
      "4000/4000 [==============================] - 633s 158ms/step - loss: 0.0058 - val_loss: 0.0312\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0047\n",
      "\tTrain r2: 0.5894785911742555\n",
      "\tValidate r2: 0.17764779649715454\n",
      "\n",
      "4000/4000 [==============================] - 637s 159ms/step - loss: 0.0047 - val_loss: 0.0300\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.0039\n",
      "\tTrain r2: 0.6146372224558271\n",
      "\tValidate r2: 0.20056151974340383\n",
      "\n",
      "4000/4000 [==============================] - 632s 158ms/step - loss: 0.0039 - val_loss: 0.0308\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    import numpy as np\n",
    "    import sys\n",
    "    import json\n",
    "\n",
    "    ATTRIBUTE = 'Trustworthiness'\n",
    "\n",
    "    ANNO = 'Annotations/' + ATTRIBUTE + '/annotations.csv'\n",
    "    TRAIN_DIR = 'Images/'+'train'\n",
    "    VAL_DIR = 'Images/'  + 'validate'\n",
    "    TEST_DIR = 'Images/' + 'test'\n",
    "\n",
    "    SPACE_FILE = 'Spaces/' + ATTRIBUTE + '_space.json'\n",
    "    MODEL_PATH = 'Models/' + ATTRIBUTE + '.h5'\n",
    "\n",
    "    print('Loading Train Data')\n",
    "    Xtrain, ytrain = load_data_into_memory(DIR=TRAIN_DIR, ANNO=ANNO, ATTRIBUTE=ATTRIBUTE, normalize=False, rollaxis=False)\n",
    "    print('Loading Train Data Again')\n",
    "    Xtrain_norm, ytrain_norm = load_data_into_memory(DIR=TRAIN_DIR, ANNO=ANNO, ATTRIBUTE=ATTRIBUTE, normalize=True, rollaxis=True)\n",
    "    print('Loading Validation Data')\n",
    "    Xvalidate, yvalidate = load_data_into_memory(DIR=VAL_DIR, ANNO=ANNO, ATTRIBUTE=ATTRIBUTE, normalize=True, rollaxis=True)\n",
    "\n",
    "    with open(SPACE_FILE, 'r') as f:\n",
    "        opt_params = json.load(f)\n",
    "        model, results, model_history = train(Xtrain, ytrain, Xtrain_norm, ytrain_norm, Xvalidate, yvalidate, opt_params)\n",
    "        model.save(MODEL_PATH)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sig_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2cd4bff2dc08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msig_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sig_history' is not defined"
     ]
    }
   ],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = sig_history.history['val_loss']\n",
    "epochs = range(1, len(loss)+ 1)\n",
    "plt.plot(epochs, loss, 'y', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "new_model = tf.keras.models.load_model('Models/' + 'IQ' + '.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest, ytest = load_data_into_memory(DIR=TEST_DIR, ANNO=ANNO, ATTRIBUTE=ATTRIBUTE, normalize=True, rollaxis=True)\n",
    "r2 = get_metrics(new_model, x=Xtest, y=ytest)\n",
    "print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
